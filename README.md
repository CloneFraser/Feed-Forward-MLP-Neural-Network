# Feed-Forward-MLP-Neural-Network
Feed-forward multi-layer perceptron neural network for classification of the MNIST and Fashion MNIST datasets

As part of my MEng degree, the report titled - coursework report.pdf, delves into the construction and analysis of perceptron neural network-based models for solving logical functions and classifying images. In the first exercise, the report explores how perceptrons and multi-layer perceptrons can be configured to solve basic logic operations, including AND, NAND, OR, and XOR, with visualizations and truth tables illustrating the separability challenges.

In the second exercise, multi-layer perceptron (MLP) models are evaluated on the MNIST handwritten digits and fashion datasets. The report assesses the impact of various parameters, including learning rate, number of hidden neurons, and training epochs, on model accuracy. Notably, the best configuration for digit classification achieved 98.19% accuracy, while the fashion dataset saw reduced accuracy due to its increased complexity.

Future improvement recommendations include transitioning to convolutional neural networks (CNNs) for enhanced feature extraction and exploring dimensionality reduction via PCA, aiming to boost classification performance on more complex datasets.

### NOTE ###
The large training files for both MNIST and Fashion MNIST are not available in this repo as they exceed 25mbs
